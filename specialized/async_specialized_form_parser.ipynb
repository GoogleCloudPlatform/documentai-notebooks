{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0J138mj7p1s"
   },
   "source": [
    "# Document AI Specialized Parser (Async)\n",
    "This notebook shows you how use Document AI's specialized parsers ex. Invoice, Receipt, and W9 asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_v0XtSwn7fmN",
    "outputId": "373b9379-f6ac-451a-e428-ad9219fb31d0"
   },
   "outputs": [],
   "source": [
    "# Install necessary Python libraries and restart your kernel after.\n",
    "!python -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8eO6Kcp7v2x"
   },
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import simplejson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your processor variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3c1mTa6IOk3"
   },
   "outputs": [],
   "source": [
    "# TODO(developer): Fill these variables with your values before running the sample\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID_HERE\"\n",
    "LOCATION = \"us\"  # Format is 'us' or 'eu'\n",
    "PROCESSOR_ID = \"PROCESSOR_ID\"  # Create processor in Cloud Console\n",
    "\n",
    "GCS_INPUT_BUCKET = 'cloud-samples-data'\n",
    "GCS_INPUT_PREFIX = 'documentai/async_invoices/'\n",
    "GCS_OUTPUT_URI = 'YOUR-OUTPUT-BUCKET'\n",
    "GCS_OUTPUT_URI_PREFIX = 'TEST'\n",
    "TIMEOUT = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code calls the synchronous API and parses the form fields and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO3yJpDoJ3Zf"
   },
   "outputs": [],
   "source": [
    "def process_document_sample():\n",
    "    # Instantiates a client\n",
    "    client_options = {\"api_endpoint\": \"{}-documentai.googleapis.com\".format(LOCATION)}\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=client_options)\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Sample invoices are stored in gs://cloud-samples-data/documentai/async_invoices/\n",
    "    blobs = storage_client.list_blobs(GCS_INPUT_BUCKET, prefix=GCS_INPUT_PREFIX)\n",
    "    input_configs = []\n",
    "    print(\"Input Files:\")\n",
    "    for blob in blobs:\n",
    "        if \".pdf\" in blob.name:\n",
    "            source = \"gs://{bucket}/{name}\".format(bucket = GCS_INPUT_BUCKET, name = blob.name)\n",
    "            print(source)\n",
    "            input_config = documentai.types.document_processor_service.BatchProcessRequest.BatchInputConfig(\n",
    "                gcs_source=source, mime_type=\"application/pdf\")\n",
    "            input_configs.append(input_config)\n",
    "\n",
    "    destination_uri = f\"{GCS_OUTPUT_URI}/{GCS_OUTPUT_URI_PREFIX}/\"\n",
    "\n",
    "    # Where to write results\n",
    "    output_config = documentai.types.document_processor_service.BatchProcessRequest.BatchOutputConfig(\n",
    "        gcs_destination=destination_uri\n",
    "    )\n",
    "\n",
    "    # The full resource name of the processor, e.g.:\n",
    "    # projects/project-id/locations/location/processor/processor-id\n",
    "    # You must create new processors in the Cloud Console first.\n",
    "    name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/processors/{PROCESSOR_ID}\"\n",
    "    request = documentai.types.document_processor_service.BatchProcessRequest(\n",
    "        name=name,\n",
    "        input_configs=input_configs,\n",
    "        output_config=output_config,\n",
    "    )\n",
    "\n",
    "    operation = client.batch_process_documents(request)\n",
    "\n",
    "    # Wait for the operation to finish\n",
    "    operation.result(timeout=TIMEOUT)\n",
    "\n",
    "    # Results are written to GCS. Use a regex to find\n",
    "    # output files\n",
    "    match = re.match(r\"gs://([^/]+)/(.+)\", destination_uri)\n",
    "    output_bucket = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "\n",
    "    bucket = storage_client.get_bucket(output_bucket)\n",
    "    blob_list = list(bucket.list_blobs(prefix=prefix))\n",
    "\n",
    "    for i, blob in enumerate(blob_list):\n",
    "        # If JSON file, download the contents of this blob as a bytes object.\n",
    "        if \".json\" in blob.name:\n",
    "            blob_as_bytes = blob.download_as_string()\n",
    "            print(\"downloaded\")\n",
    "\n",
    "            document = documentai.types.Document.from_json(blob_as_bytes)\n",
    "            print(f\"Fetched file {i + 1}\")\n",
    "\n",
    "            # For a full list of Document object attributes, please reference this page:\n",
    "            # https://cloud.google.com/document-ai/docs/reference/rpc/google.cloud.documentai.v1beta3#document\n",
    "            \n",
    "            # Read the entities output from the processor\n",
    "            types = []\n",
    "            values = []\n",
    "            confidence = []\n",
    "            \n",
    "            for entity in document.entities:\n",
    "                types.append(entity.type_)\n",
    "                values.append(entity.mention_text)\n",
    "                confidence.append(round(entity.confidence,4))\n",
    "        \n",
    "            # Create a Pandas Dataframe to print the values in tabular format. \n",
    "            df = pd.DataFrame({'Type': types, 'Value': values, 'Confidence': confidence}) \n",
    "            display(df)\n",
    "                \n",
    "        else:\n",
    "            print(f\"Skipping non-supported file type {blob.name}\")\n",
    "\n",
    "\n",
    "# Extract shards from the text field\n",
    "def get_text(doc_element: dict, document: dict):\n",
    "    \"\"\"\n",
    "    Document AI identifies form fields by their offsets\n",
    "    in document text. This function converts offsets\n",
    "    to text snippets.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    # If a text segment spans several lines, it will\n",
    "    # be stored in different text segments.\n",
    "    for segment in doc_element.text_anchor.text_segments:\n",
    "        start_index = (\n",
    "            int(segment.start_index)\n",
    "            if segment in doc_element.text_anchor.text_segments\n",
    "            else 0\n",
    "        )\n",
    "        end_index = int(segment.end_index)\n",
    "        response += document.text[start_index:end_index]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = process_document_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LendingAI Bouding Boxes v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
